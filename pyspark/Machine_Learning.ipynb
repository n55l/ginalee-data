{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext(appName=\"YourTest\", master=\"local[*]\")"
      ],
      "metadata": {
        "id": "mvVvU6WgpusM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spamminess import spamminess\n",
        "from math import exp\n",
        "\n",
        "def sequential_SGD(model, training_dataset='spam.train.group_x.txt', delta = 0.002):\n",
        "    #### Your solution to Question 1 here\n",
        "    # open one of the training files - defaults to group_x\n",
        "    \n",
        "    with open(training_dataset) as f:\n",
        "        for line in f:\n",
        "          doc = line.split()\n",
        "          t = doc[1]\n",
        "          F = doc[2:]\n",
        "          F = [int(x) for x in F]\n",
        "\n",
        "          score = spamminess(F,model)\n",
        "\n",
        "          prob = 1.0/(1+exp(-score))\n",
        "          for f in F:\n",
        "            if t == 'spam':\n",
        "              if f in model:\n",
        "                model[f] += (1.0-prob)*delta\n",
        "              else:\n",
        "                model[f] = (1.0-prob)*delta\n",
        "            elif t == 'ham':\n",
        "              if f in model:\n",
        "               model[f] -= prob*delta\n",
        "              else:\n",
        "                model[f] = - prob*delta\n",
        "\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "-i102TEMf2zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spamminess import spamminess\n",
        "from math import exp\n",
        "import shutil, os\n",
        "\n",
        "def spark_SGD(training_dataset='spam.train.group_x.txt', output_model='models/group_x_model', delta = 0.002):\n",
        "    if os.path.isdir(output_model):\n",
        "        shutil.rmtree(output_model) # Remove the previous model to create a new one\n",
        "    \n",
        "    training_data = sc.textFile(training_dataset)\n",
        "\n",
        "    train = training_data.map(lambda x: x.split())\\\n",
        "                         .map(lambda y: y[1:])\\\n",
        "                         .coalesce(1)\n",
        "\n",
        "\n",
        "    def f(iterator):\n",
        "      w = {}\n",
        "\n",
        "      for doc in iterator:\n",
        "        t = doc[0]\n",
        "        F = doc[1:]\n",
        "        F = [int(x) for x in F]\n",
        "\n",
        "        score = spamminess(F,w)     \n",
        "        prob = 1.0/(1+exp(-score))\n",
        "          \n",
        "\n",
        "        for f in F:\n",
        "          if t == 'spam':\n",
        "            if f in w:\n",
        "              w[f] += (1.0-prob)*0.002\n",
        "            else:\n",
        "              w[f] = (1.0-prob)*0.002\n",
        "          elif t == 'ham':\n",
        "            if f in w:\n",
        "              w[f] -= prob*0.002\n",
        "            else:\n",
        "              w[f] = - prob*0.002\n",
        "\n",
        "      yield w\n",
        "\n",
        "    final = train.mapPartitions(f)\\\n",
        "                 .map(lambda x: [(k, v) for k, v in x.items()])\\\n",
        "                 .flatMap(lambda x: x)\n",
        "\n",
        "    final.saveAsTextFile(output_model)\n"
      ],
      "metadata": {
        "id": "SnhtthRTqvXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spamminess import spamminess\n",
        "from math import exp\n",
        "import shutil, os, random\n",
        "import random\n",
        "\n",
        "def spark_shuffled_SGD(training_dataset='spam.train.group_x.txt', output_model='models/group_x_model', delta = 0.002):\n",
        "    if os.path.isdir(output_model):\n",
        "        shutil.rmtree(output_model) # Remove the previous model to create a new one\n",
        "\n",
        "    training_data = sc.textFile(training_dataset)\n",
        "\n",
        "    train = training_data.map(lambda x: x.split())\\\n",
        "                         .map(lambda y: y[1:])\\\n",
        "                         .coalesce(1)\n",
        "\n",
        "    lines = train.count()\n",
        "\n",
        "    def shuffle(iterator):\n",
        "      indexed = []\n",
        "      a = random.sample(range(lines), lines)\n",
        "      i = 0\n",
        "      for doc in iterator:\n",
        "       indexed.append([a[i]] + [doc])\n",
        "       i += 1\n",
        "      yield indexed\n",
        "\n",
        "    shuffled_train = train.mapPartitions(shuffle)\\\n",
        "                          .flatMap(lambda x: x)\\\n",
        "                          .repartition(lines)\\\n",
        "                          .sortByKey()\\\n",
        "                          .values()\\\n",
        "                          .coalesce(1)\n",
        "\n",
        "    def f(iterator):\n",
        "      w = {}\n",
        "\n",
        "      for doc in iterator:\n",
        "        t = doc[0]\n",
        "        F = doc[1:]\n",
        "        F = [int(x) for x in F]\n",
        "\n",
        "        score = spamminess(F,w)     \n",
        "        prob = 1.0/(1+exp(-score))\n",
        "          \n",
        "\n",
        "        for f in F:\n",
        "          if t == 'spam':\n",
        "            if f in w:\n",
        "              w[f] += (1.0-prob)*0.002\n",
        "            else:\n",
        "              w[f] = (1.0-prob)*0.002\n",
        "          elif t == 'ham':\n",
        "            if f in w:\n",
        "              w[f] -= prob*0.002\n",
        "            else:\n",
        "              w[f] = - prob*0.002\n",
        "\n",
        "      yield w\n",
        "\n",
        "    final = shuffled_train.mapPartitions(f)\\\n",
        "                          .map(lambda x: [(k, v) for k, v in x.items()])\\\n",
        "                          .flatMap(lambda x: x)\n",
        "\n",
        "    final.saveAsTextFile(output_model)"
      ],
      "metadata": {
        "id": "LU4I_jwqgF0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spamminess import spamminess\n",
        "import shutil, os\n",
        "\n",
        "def spark_classify(input_model='models/group_x_model', test_dataset='spam.test.qrels.txt', results_path='results/test_qrels'):\n",
        "    if os.path.isdir(results_path):\n",
        "        shutil.rmtree(results_path) # Remove the previous results\n",
        "\n",
        "    test_data = sc.textFile(test_dataset)\n",
        "\n",
        "    model_path = '{}/part-00000'.format(input_model)\n",
        "    model = []\n",
        "    with open(model_path) as f:\n",
        "      for line in f:\n",
        "        model.append(eval(line.strip()))\n",
        "\n",
        "    model = dict(model)\n",
        "    result = test_data.map(lambda x: x.split())\\\n",
        "                      .map(lambda x: x[:2] + [spamminess([int(i) for i in x[2:]], model)])\\\n",
        "                      .map(lambda x: x+['spam'] if x[-1]>0 else x+['ham'])\\\n",
        "                      .map(lambda x: tuple(x))\n",
        "\n",
        "    result.saveAsTextFile(results_path)"
      ],
      "metadata": {
        "id": "0AwcXpd4gKdk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}