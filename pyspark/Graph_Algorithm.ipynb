{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvygKanufcop"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"YourTest\")\\\n",
        "  .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_nodes_edges():\n",
        "    \"\"\"Returns a tuple (num_nodes, num_edges)\"\"\"\n",
        "    #### Your code for Question 1.1 should go here\n",
        "\n",
        "    text = sc.textFile('p2p-Gnutella08-adj.txt')\n",
        "    adjacency_list = text.map(lambda x: x.split())\\\n",
        "                         .map(lambda text_lst: [int(text) for text in text_lst])\\\n",
        "                         .map(lambda lst: (lst[0], lst[1:]))\\\n",
        "                         .flatMapValues(lambda x: x)\\\n",
        "                         .map(lambda s : tuple(sorted(s)))\\\n",
        "                         .distinct()                     \n",
        "\n",
        "    num_nodes = text.count()                   \n",
        "    num_edges = adjacency_list.count()                 \n",
        "    return (num_nodes, num_edges)\n",
        "    \n",
        "    \n",
        "def out_counts():\n",
        "    \"\"\"Returns a dictionary where the keys are the outdegrees, and the \n",
        "    values are the number of nodes of the corresponding outdegree \"\"\"\n",
        "    #### Your code for Question 1.2 should go here\n",
        "    text = sc.textFile('p2p-Gnutella08-adj.txt')\n",
        "    out_count_dict = text.map(lambda x: x.split())\\\n",
        "                         .map(lambda text_lst: [int(text) for text in text_lst])\\\n",
        "                         .map(lambda lst: (lst[0], lst[1:]))\\\n",
        "                         .mapValues(len)\\\n",
        "                         .values()\\\n",
        "                         .map(lambda out : (out, 1))\\\n",
        "                         .reduceByKey(lambda x, y: x+y)\n",
        "                         \n",
        "    return out_count_dict.collectAsMap() \n",
        "    \n",
        "\n",
        "\n",
        "def in_counts():\n",
        "    \"\"\"Returns a dictionary where the keys are the indegrees, and the \n",
        "    values are the number of nodes of the corresponding indegree \"\"\"\n",
        "    #### Your code for Question 1.3 should go here\n",
        "    text = sc.textFile('p2p-Gnutella08-adj.txt')\n",
        "    in_count_dict = text.map(lambda x: x.split())\\\n",
        "                        .map(lambda text_lst: [int(text) for text in text_lst])\\\n",
        "                        .map(lambda lst: (lst[0], lst[1:]))\\\n",
        "                        .flatMapValues(lambda x: x)\\\n",
        "                        .map(lambda pair: pair[::-1])\\\n",
        "                        .groupByKey()\\\n",
        "                        .mapValues(len)\\\n",
        "                        .values()\\\n",
        "                        .map(lambda out : (out, 1))\\\n",
        "                        .reduceByKey(lambda x, y: x+y)\n",
        "\n",
        "    return in_count_dict.collectAsMap()"
      ],
      "metadata": {
        "id": "-i102TEMf2zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from functools import reduce\n",
        "\n",
        "def personalized_page_rank(source_node_id, num_iterations, jump_factor):\n",
        "    \"\"\"Returns a list of the 10 nodes with the highest page rank value along with their value, as tuples\n",
        "    [(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]\"\"\"\n",
        "    # your solution to Question 2 here\n",
        "    text = sc.textFile('p2p-Gnutella08-adj.txt')\n",
        "\n",
        "    \n",
        "    adjacency_list = text.map(lambda x: x.split())\\\n",
        "                         .map(lambda text_lst: [int(text) for text in text_lst])\\\n",
        "                         .map(lambda lst: (lst[0], lst[1:]))\\\n",
        "                         .cache()\n",
        "\n",
        "\n",
        "    # out_importance has ((out_nei, in_neigh), beta*importance)\n",
        "\n",
        "    out_importance = adjacency_list.filter(lambda x: len(x[1]) != 0)\\\n",
        "                                   .map(lambda x: (x[0], x[1], 1/len(x[1])))\\\n",
        "                                   .flatMap(lambda x : [( (i, x[0]) , (1-jump_factor) * x[2] ) for i in x[1]])\\\n",
        "\n",
        "\n",
        "    # added (1-beta) to the source_node\n",
        "    # ( in-nei, (out_nei, importance) )\n",
        "    random_jump = adjacency_list.keys()\\\n",
        "                                .map(lambda x : ((source_node_id, x), jump_factor))\\\n",
        "                                .union(out_importance)\\\n",
        "                                .reduceByKey(lambda x, y: x+y)\\\n",
        "                                .map(lambda x : (x[0][1], (x[0][0], x[1])))\\\n",
        "                                .groupByKey()\\\n",
        "                                .mapValues(list)\n",
        "\n",
        "\n",
        "\n",
        "    # solve dead ends\n",
        "\n",
        "    dead_ends = adjacency_list.filter(lambda x : len(x[1]) == 0).keys().collect()\n",
        "\n",
        "    \n",
        "    # final_matrix adds prob mass lost by dead ends\n",
        "    # ( in_nei, (out_nei, importance) )\n",
        "\n",
        "    final_matrix = random_jump.map(lambda x: (x[0], [( i[0],i[1] + 1 - sum(a[1] for a in x[1]) ) if i[0] == source_node_id else i for i in x[1]])\\\n",
        "                                   if x[0] in dead_ends else x)\\\n",
        "                              .flatMap(lambda x : [ ( x[0], (i[0], i[1]) ) for i in x[1] ])\\\n",
        "\n",
        "\n",
        "\n",
        "    rank_val = adjacency_list.keys()\\\n",
        "                              .map(lambda x : (x, 0) if x != source_node_id else (x, 1))\\\n",
        "\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "      rank_val = final_matrix.join(rank_val)\\\n",
        "                             .map(lambda x: (x[1][0][0], x[1][0][1]*x[1][1]))\\\n",
        "                             .reduceByKey(lambda x,y: x+y)\\\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return rank_val.sortBy(lambda x: x[1], ascending = False).take(10)"
      ],
      "metadata": {
        "id": "LU4I_jwqgF0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def personalized_page_rank_stopping_criterion(source_node_id, jump_factor):\n",
        "    \"\"\"Returns a list of the 10 nodes with the highest page rank value along with their value, as tuples\n",
        "    [(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]\"\"\"\n",
        "    # your solution to Question 3 here\n",
        "    text = sc.textFile('p2p-Gnutella08-adj.txt')\n",
        "\n",
        "    \n",
        "    adjacency_list = text.map(lambda x: x.split())\\\n",
        "                         .map(lambda text_lst: [int(text) for text in text_lst])\\\n",
        "                         .map(lambda lst: (lst[0], lst[1:]))\\\n",
        "                         .cache()\n",
        "\n",
        "\n",
        "    # out_importance has ((out_nei, in_neigh), beta*importance)\n",
        "\n",
        "    out_importance = adjacency_list.filter(lambda x: len(x[1]) != 0)\\\n",
        "                                   .map(lambda x: (x[0], x[1], 1/len(x[1])))\\\n",
        "                                   .flatMap(lambda x : [( (i, x[0]) , (1-jump_factor) * x[2] ) for i in x[1]])\n",
        "\n",
        "\n",
        "\n",
        "    # added (1-beta) to the source_node\n",
        "    # ( in-nei, (out_nei, importance) )\n",
        "\n",
        "    random_jump = adjacency_list.keys()\\\n",
        "                                .map(lambda x : ((source_node_id, x), jump_factor))\\\n",
        "                                .union(out_importance)\\\n",
        "                                .reduceByKey(lambda x, y: x+y)\\\n",
        "                                .map(lambda x : (x[0][1], (x[0][0], x[1])))\\\n",
        "                                .groupByKey()\\\n",
        "                                .mapValues(list)\\\n",
        "                                .cache()\n",
        "\n",
        "\n",
        "\n",
        "    # solve dead ends\n",
        "\n",
        "    dead_ends = adjacency_list.filter(lambda x : len(x[1]) == 0).keys().collect()\n",
        "\n",
        "    \n",
        "    # final_matrix adds prob mass lost by dead ends\n",
        "    # ( in_nei, (out_nei, importance) )\n",
        "\n",
        "    final_matrix = random_jump.map(lambda x: (x[0], [( i[0],i[1] + 1 - sum(a[1] for a in x[1]) ) if i[0] == source_node_id else i for i in x[1]])\\\n",
        "                                   if x[0] in dead_ends else x)\\\n",
        "                              .flatMap(lambda x : [ ( x[0], (i[0], i[1]) ) for i in x[1] ])\\\n",
        "\n",
        "\n",
        "\n",
        "    rank_val = adjacency_list.keys()\\\n",
        "                              .map(lambda x : (x, 0) if x != source_node_id else (x, 1))\\\n",
        "\n",
        "    N = adjacency_list.count()\n",
        "    max_rank_diff = 0.5/N\n",
        "\n",
        "                            \n",
        "    while max_rank_diff >= 0.5/N:\n",
        "      prev_rank_val = rank_val\n",
        "      rank_val = final_matrix.join(prev_rank_val)\\\n",
        "                             .map(lambda x: (x[1][0][0], x[1][0][1]*x[1][1]))\\\n",
        "                             .reduceByKey(lambda x,y: x+y)\\\n",
        "      \n",
        "      max_rank_diff = rank_val.union(prev_rank_val)\\\n",
        "                              .reduceByKey(lambda x,y: abs(x-y))\\\n",
        "                              .values()\\\n",
        "                              .max()\n",
        "\n",
        "\n",
        "    return rank_val.sortBy(lambda x: x[1], ascending = False).take(10)"
      ],
      "metadata": {
        "id": "0AwcXpd4gKdk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}