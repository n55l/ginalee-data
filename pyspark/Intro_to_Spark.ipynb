{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvygKanufcop"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"YourTest\")\\\n",
        "  .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "\n",
        "# Returns the count of distinct tokens in the `Shakespeare.txt` dataset\n",
        "def count_distinct_tokens():\n",
        "    # your solution to Question 2 here\n",
        "  lines = sc.textFile('Shakespeare.txt')\n",
        "  count = lines.flatMap(lambda line: simple_tokenize(line))\\\n",
        "               .distinct()\\\n",
        "               .count()\n",
        "  return count\n",
        "count_distinct_tokens()"
      ],
      "metadata": {
        "id": "-i102TEMf2zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "import itertools\n",
        "\n",
        "# Returns the count of distinct pairs in the `Shakespeare.txt` dataset\n",
        "def count_distinct_pairs():\n",
        "    # your solution to Question 3 here\n",
        "  lines = sc.textFile('Shakespeare.txt')\n",
        "  pairs = lines.map(lambda line: simple_tokenize(line))\\\n",
        "               .map(lambda lst: list(set(lst)))\\\n",
        "               .flatMap(lambda lst: itertools.permutations(lst, 2))\\\n",
        "               .distinct()\n",
        "\n",
        "  return pairs.count()"
      ],
      "metadata": {
        "id": "LU4I_jwqgF0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "\n",
        "# Returns a list of the top 50 (probability, count, token) tuples, ordered by probability\n",
        "def top_50_tokens_probabilities():\n",
        "    # your solution to Question 4 here \n",
        "\n",
        "  lines = sc.textFile('Shakespeare.txt')\n",
        "  lines_count = lines.count()\n",
        "  prob = lambda token_count: token_count[1]/lines_count\n",
        "  swap = lambda x: (x[2], x[1], x[0])\n",
        "  token_count_prob = lines.map(lambda line: simple_tokenize(line))\\\n",
        "                          .flatMap(lambda lst: list(set(lst)))\\\n",
        "                          .map(lambda word: (word, 1))\\\n",
        "                          .reduceByKey(lambda x,y: x+y)\\\n",
        "                          .map(lambda x: x+(prob(x),))\\\n",
        "                          .map(swap)\\\n",
        "                          .sortBy(lambda tuple: tuple[0],False)\\\n",
        "                          .take(50)\n",
        "\n",
        "  return token_count_prob"
      ],
      "metadata": {
        "id": "0AwcXpd4gKdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "from math import log\n",
        "import itertools\n",
        "\n",
        "# Returns a list of tuples with the following format:\n",
        "# ((token1, token2), pmi, co-occurrence_count, token1_count, token2_count)\n",
        "def PMI(threshold):\n",
        "    # your solution to Question 5 here\n",
        "  lines = sc.textFile('Shakespeare.txt')\n",
        "  lines_count = lines.count()\n",
        "  prob = lambda pair_count: pair_count[1]/lines_count\n",
        "\n",
        "  tokens_count_prob = lines.map(lambda line: simple_tokenize(line))\\\n",
        "                           .flatMap(lambda lst: list(set(lst)))\\\n",
        "                           .map(lambda word: (word, 1))\\\n",
        "                           .reduceByKey(lambda x,y: x+y)\\\n",
        "                           .map(lambda a: (a[0], (a[1], prob(a))))\n",
        "\n",
        "# first .join(tokens_count_prob) created the form of (token1, ((token2, co-occurrence_count, p(x,y)), (token1_count, p(x))))\n",
        "# second .join(tokens_count_prob) created the form of (token2, (token1, co-occurrence_count, p(x,y), token1_count, p(x), token2_count, p(y)))\n",
        "\n",
        "  pairs_count = lines.map(lambda line: simple_tokenize(line))\\\n",
        "                     .map(lambda lst: list(set(lst)))\\\n",
        "                     .flatMap(lambda lst: itertools.permutations(lst, 2))\\\n",
        "                     .map(lambda pair: (pair, 1))\\\n",
        "                     .reduceByKey(lambda x,y: x+y)\\\n",
        "                     .filter(lambda pair_count: pair_count[1] >= threshold)\\\n",
        "                     .map(lambda x: x+(prob(x),))\\\n",
        "                     .map(lambda t: (t[0][0], (t[0][1], t[1], t[2])))\\\n",
        "                     .join(tokens_count_prob)\\\n",
        "                     .map(lambda t: (t[1][0][0], (t[0],) + t[1][0][1:] + t[1][1]))\\\n",
        "                     .join(tokens_count_prob)\\\n",
        "                     .map(lambda y: ((y[1][0][0], y[0]), y[1][0][1:] + y[1][1]))  # till here we have ((token1,token2), (co-occurrence_count, p(x,y), token1_count, p(x), token2_count, p(y)))\n",
        "  \n",
        "  pmi = lambda a: log(a[1][1] / (a[1][3] * a[1][5]))\n",
        "  # (token1, token2), pmi, co-occurrence_count, token1_count, token2_count\n",
        "  pmi_pair = pairs_count.map(lambda x: (x[0], pmi(x), x[1][0], x[1][2], x[1][4]))\n",
        "\n",
        "  return pmi_pair.collect()"
      ],
      "metadata": {
        "id": "6OtRt98bgSeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "from math import log\n",
        "\n",
        "# Returns a list of samp_size tuples with the following format:\n",
        "# (token, [ list_of_cooccurring_tokens ])\n",
        "# where list_of_cooccurring_tokens is of the form\n",
        "# [((token1, token2), pmi, cooc_count, token1_count, token2_count), ...]\n",
        "def PMI_one_token(threshold, samp_size):\n",
        "    # your solution to Question 6 here\n",
        "\n",
        "  lines = sc.textFile('Shakespeare.txt')\n",
        "  lines_count = lines.count()\n",
        "  prob = lambda pair_count: pair_count[1]/lines_count\n",
        "\n",
        "  tokens_count_prob = lines.map(lambda line: simple_tokenize(line))\\\n",
        "                           .flatMap(lambda lst: list(set(lst)))\\\n",
        "                           .map(lambda word: (word, 1))\\\n",
        "                           .reduceByKey(lambda x,y: x+y)\\\n",
        "                           .map(lambda a: (a[0], (a[1], prob(a))))\n",
        "\n",
        "# first .join(tokens_count_prob) created the form of (token1, ((token2, co-occurrence_count, p(x,y)), (token1_count, p(x))))\n",
        "# second .join(tokens_count_prob) created the form of (token2, (token1, co-occurrence_count, p(x,y), token1_count, p(x), token2_count, p(y)))\n",
        "\n",
        "  pairs_count = lines.map(lambda line: simple_tokenize(line))\\\n",
        "                     .map(lambda lst: list(set(lst)))\\\n",
        "                     .flatMap(lambda lst: itertools.permutations(lst, 2))\\\n",
        "                     .map(lambda pair: (pair, 1))\\\n",
        "                     .reduceByKey(lambda x,y: x+y)\\\n",
        "                     .filter(lambda pair_count: pair_count[1] >= threshold)\\\n",
        "                     .map(lambda x: x+(prob(x),))\\\n",
        "                     .map(lambda t: (t[0][0], (t[0][1], t[1], t[2])))\\\n",
        "                     .join(tokens_count_prob)\\\n",
        "                     .map(lambda t: (t[1][0][0], (t[0],) + t[1][0][1:] + t[1][1]))\\\n",
        "                     .join(tokens_count_prob)\\\n",
        "                     .map(lambda y: ((y[1][0][0], y[0]), y[1][0][1:] + y[1][1]))  # till here we have ((token1,token2), (co-occurrence_count, p(x,y), token1_count, p(x), token2_count, p(y)))\n",
        "  \n",
        "  pmi = lambda a: log(a[1][1] / (a[1][3] * a[1][5]))\n",
        "  # (token1, token2), pmi, co-occurrence_count, token1_count, token2_count\n",
        "  pmi_pair = pairs_count.map(lambda x: (x[0], pmi(x), x[1][0], x[1][2], x[1][4]))\n",
        "\n",
        "\n",
        "  # pull t1 as key\n",
        "  pmi_samp = pmi_pair.map(lambda x: (x[0][0],(x)))\\\n",
        "                     .groupByKey()\\\n",
        "                     .mapValues(lambda tuples: [t for t in tuples if t])\\\n",
        "                     .takeSample(False, samp_size)\n",
        "\n",
        "  return pmi_samp"
      ],
      "metadata": {
        "id": "QXjyfMUXgTQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}