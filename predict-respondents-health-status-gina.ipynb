{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pog\n# oh yea we pog\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom math import floor\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split, GridSearchCV \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, StackingClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils\nfrom keras.losses import CategoricalCrossentropy\n\nfolder = '/kaggle/input/stat-441-w2022-data-challenge/'\n\n# We get the necessary data\ntrain_data = pd.read_csv(folder+'train.csv') # training data\ntest_data = pd.read_csv(folder+'test.csv') # test data\nsample_sub = pd.read_csv(folder+'sample_submission.csv')\ntrain_data.sample(5)\n# train_data.info(verbose=True, show_counts=True)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-02T19:36:49.036643Z","iopub.execute_input":"2022-04-02T19:36:49.037406Z","iopub.status.idle":"2022-04-02T19:36:52.157732Z","shell.execute_reply.started":"2022-04-02T19:36:49.037355Z","shell.execute_reply":"2022-04-02T19:36:52.156833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To begin with this competition, we create a function that can provide us a training set and testing set depending on the dataframe. We can further change the size of the training and testing sets through the optional parameter.","metadata":{}},{"cell_type":"code","source":"def train_test_sets(X,y,train_size=0.75):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=train_size, random_state=441)\n    return (X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:34:11.361399Z","iopub.execute_input":"2022-04-02T19:34:11.36186Z","iopub.status.idle":"2022-04-02T19:34:11.366687Z","shell.execute_reply.started":"2022-04-02T19:34:11.361826Z","shell.execute_reply":"2022-04-02T19:34:11.365753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now clean the dataframe to get rid of null values. We will using two approaches for data cleaning. In the first approach, we will drop the columns if there are any columns with less than 50% non-NA values. In the second approach, we use 60%. We believe that using this approach can give an idea of how the sizes can affect the accuracy, and subsequently, entropy as well.  \n\nIn both the approaches, we will impute the remaining columns with their respective modes. In the appendix, we have explained more about the reason behind using \"mode\" for imputation.  ","metadata":{}},{"cell_type":"code","source":"train_data_clean_1 = train_data.dropna(axis = 1, thresh = 8698) # drop columns with 50% or more null values\ntrain_data_clean_1.drop(['uniqueid','personid'], axis=1, inplace=True) # drop the unnecessary columns\nfor name in train_data_clean_1.columns:\n    # impute the columns with mode\n    train_data_clean_1.loc[train_data_clean_1[name].isnull(), name] = train_data_clean_1[name].mode()[0]\nprint(train_data_clean_1.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:34:13.469928Z","iopub.execute_input":"2022-04-02T19:34:13.470843Z","iopub.status.idle":"2022-04-02T19:34:13.712692Z","shell.execute_reply.started":"2022-04-02T19:34:13.470798Z","shell.execute_reply":"2022-04-02T19:34:13.711852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression approach  \n","metadata":{}},{"cell_type":"code","source":"# X = train_data_clean_1.iloc[:,:-1]\n# y = train_data_clean_1['health']\n# X_train, X_test, y_train, y_test = train_test_sets(X,y, 0.8)\n\n# params = {'penalty':['l2','elasticnet','none'], 'C': [0.1, 0.5, 1],\n#          'warm_start':[True, False], 'l1_ratio':[0, 0.1, 0.5, 0.75, 1]}\n# logmod = LogisticRegression(multi_class=\"multinomial\", max_iter=1000, solver='saga')\n# log = GridSearchCV(logmod, params, n_jobs=-1, verbose=0, refit='neg_log_loss', scoring=['accuracy','neg_log_loss'])\n# log.fit(X_train, y_train)\n# print(log.best_estimator_, log.best_score_, log.best_params_, sep='\\n')\n# log.score(X_test, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:34:41.136639Z","iopub.execute_input":"2022-03-30T02:34:41.136885Z","iopub.status.idle":"2022-03-30T02:34:41.140857Z","shell.execute_reply.started":"2022-03-30T02:34:41.136857Z","shell.execute_reply":"2022-03-30T02:34:41.13995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LogisticRegression(C=0.1, l1_ratio=0.1, max_iter=1000,\n                   multi_class='multinomial', solver='saga', warm_start=True)  \n                   \n-1.3887890226274735  \n\n{'C': 0.1, 'l1_ratio': 0.1, 'penalty': 'l2', 'warm_start': True}  \n\n-1.3998282372992474","metadata":{}},{"cell_type":"markdown","source":"# XGB Classifier","metadata":{}},{"cell_type":"code","source":"# X = train_data.iloc[:,3:-1]\n# y = train_data['health']\n# X_train, X_test, y_train, y_test = train_test_sets(X,y, 0.8)\n\n# encoder = LabelEncoder()\n# encoder.fit(y_train)\n# ytrain = encoder.transform(y_train)\n\n# encoder.fit(y_test)\n# ytest = encoder.transform(y_test)\n\n# params = {'n_estimators':[20, 50, 75, 100, 120], 'learning_rate': (0.1, 0.15, 0.2, 0.25, 0.3)}\n# xgb = XGBClassifier(objective='multi:softprob', n_jobs=-1, eval_metric='mlogloss', \\\n#                     max_depth=2, verbosity=0, missing=np.nan)\n# xgb = GridSearchCV(xgb, params, n_jobs=-1, verbose=0, refit='neg_log_loss', scoring=['accuracy','neg_log_loss'])\n# xgb.fit(X_train, y_train)\n# print(xgb.best_estimator_, xgb.best_score_, xgb.best_params_, sep='\\n')\n# xgb.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:34:41.142102Z","iopub.execute_input":"2022-03-30T02:34:41.142432Z","iopub.status.idle":"2022-03-30T02:34:41.155962Z","shell.execute_reply.started":"2022-03-30T02:34:41.142404Z","shell.execute_reply":"2022-03-30T02:34:41.155162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best estimator is `XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              eval_metric='mlogloss', gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n              max_depth=2, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=100, n_jobs=-1,\n              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n              subsample=1, tree_method='exact', validate_parameters=1,\n              verbosity=0)`  \nScores and params are `-1.214446512794508 {'learning_rate': 0.2, 'n_estimators': 100}`","metadata":{}},{"cell_type":"code","source":"# X = train_data_clean_1.iloc[:,:-1]\n# y = train_data_clean_1['health']\n# X_train, X_test, y_train, y_test = train_test_sets(X,y, 0.8)\n\n# params = {'gamma': [0, 0.5, 1, 2, 5, 10, 20, 50], 'max_depth':[1, 2, 5, 10], 'eta':[0.1, 0.2, 0.5, 0.8, 1],\\\n#          'max_delta_step':[1, 2, 5, 10], 'subsample':[0.5, 0.75, 1], 'reg_lambda':[0, 0.2, 0.5, 0.75, 1],\\\n#          'reg_alpha':[0, 0.2, 0.5, 0.75, 1], 'tree_method':['exact','hist','gpu_hist','approx', 'auto']}\n# xgb_model = XGBClassifier(objective='multi:softprob', verbosity=0, eval_metric='mlogloss')\n# gs_cv = GridSearchCV(xgb_model, params, refit=False, scoring=['accuracy', 'neg_log_loss'], verbose=0, n_jobs=-1)\n\n# gs_cv.fit(X_train, y_train)\n# print(gs_cv.best_score_, gs_cv.best_estimator_)\n# print(gs_cv.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:34:41.157148Z","iopub.execute_input":"2022-03-30T02:34:41.157877Z","iopub.status.idle":"2022-03-30T02:34:41.167441Z","shell.execute_reply.started":"2022-03-30T02:34:41.157833Z","shell.execute_reply":"2022-03-30T02:34:41.166441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdaBoost + XGBClassifer  \n\n**Too slow hence should never be executed**","metadata":{}},{"cell_type":"code","source":"# X = train_data_clean_1.iloc[:,:-1]\n# y = train_data_clean_1['health']\n# X_train, X_test, y_train, y_test = train_test_sets(X,y, 0.8)\n# params = {'n_estimators':[1, 10, 20, 50, 100], 'learning_rate':[0.5, 1, 1.75, 2.5, 3]}\n# adaModel = AdaBoostClassifier(XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', max_depth=2),random_state=441)\n# clf = GridSearchCV(adaModel, params, n_jobs=-1, verbose=0, scoring='accuracy')\n# clf.fit(X_train, y_train)\n# print(clf.best_params_, clf.best_score_)\n# clf.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:34:41.168603Z","iopub.execute_input":"2022-03-30T02:34:41.169369Z","iopub.status.idle":"2022-03-30T02:34:41.184545Z","shell.execute_reply.started":"2022-03-30T02:34:41.169334Z","shell.execute_reply":"2022-03-30T02:34:41.183653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdaBoost","metadata":{}},{"cell_type":"code","source":"# X = train_data_clean_1.iloc[:,:-1]\n# y = train_data_clean_1['health']\n# X_train, X_test, y_train, y_test = train_test_sets(X,y, 0.8)\n\n# params = {'n_estimators':[1, 10, 20, 50, 100], 'learning_rate':[0.5, 1, 1.75, 2.5, 3]}\n# ada_model = AdaBoostClassifier(random_state=441)\n# ada = GridSearchCV(ada_model, params, n_jobs=-1, verbose=0, refit='neg_log_loss', scoring=['accuracy','neg_log_loss'])\n# ada.fit(X_train, y_train)\n# print(ada.best_estimator_, ada.best_score_, ada.best_params_)\n# ada.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:34:41.18607Z","iopub.execute_input":"2022-03-30T02:34:41.186677Z","iopub.status.idle":"2022-03-30T02:34:41.196958Z","shell.execute_reply.started":"2022-03-30T02:34:41.186627Z","shell.execute_reply":"2022-03-30T02:34:41.196187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best estimator is `AdaBoostClassifier(learning_rate=0.5, n_estimators=1, random_state=441)`  \nScore and params are `1.3351385752744558 {'learning_rate': 0.5, 'n_estimators': 1}`","metadata":{}},{"cell_type":"markdown","source":"# DecisionTreeClassifier","metadata":{}},{"cell_type":"code","source":"# X = train_data_clean_1.iloc[:,:-1]\n# y = train_data_clean_1['health']\n# X_train, X_test, y_train, y_test = train_test_sets(X,y, 0.8)\n\n# params = {'max_depth':[1, 2, 5, 10, 15], 'min_samples_split': [2, 5, 10, 15, 20], 'min_impurity_decrease':[0.25, 0.5, 0.75]}\n# dtc_model = DecisionTreeClassifier(criterion='entropy', random_state=441, max_features='auto')\n# dtc = GridSearchCV(dtc_model, params, n_jobs=-1, verbose=0, refit='neg_log_loss', scoring=['accuracy','neg_log_loss'])\n# dtc.fit(X_train, y_train)\n# print(dtc.best_estimator_, dtc.best_score_, dtc.best_params_)\n# dtc.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:34:41.198327Z","iopub.execute_input":"2022-03-30T02:34:41.198909Z","iopub.status.idle":"2022-03-30T02:34:41.209125Z","shell.execute_reply.started":"2022-03-30T02:34:41.198867Z","shell.execute_reply":"2022-03-30T02:34:41.208286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best estimator is `DecisionTreeClassifier(criterion='entropy', max_depth=1, max_features='auto',\n                       min_impurity_decrease=0.25, random_state=441)`  \nBest score and params are `-1.3915510004920844 {'max_depth': 1, 'min_impurity_decrease': 0.25, 'min_samples_split': 2}`  ","metadata":{}},{"cell_type":"markdown","source":"# AdaBoost + DecisionTree ","metadata":{}},{"cell_type":"code","source":"# X = train_data_clean_1.iloc[:,:-1]\n# y = train_data_clean_1['health']\n# X_train, X_test, y_train, y_test = train_test_sets(X,y, 0.8)\n\n# params = {'n_estimators':[1, 2, 5, 10], 'learning_rate':[0.1, 0.2, 0.5, 0.75]}\n# ada_dtc_model = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=1, max_features='auto',\n#                        min_impurity_decrease=0.25, random_state=441),random_state=441)\n# ada_dtc = GridSearchCV(ada_dtc_model, params, n_jobs=-1, verbose=0, refit='neg_log_loss', scoring=['accuracy','neg_log_loss'])\n# ada_dtc.fit(X_train, y_train)\n# print(ada_dtc.best_estimator_, ada_dtc.best_score_, ada_dtc.best_params_)\n# ada_dtc.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:34:41.212009Z","iopub.execute_input":"2022-03-30T02:34:41.212753Z","iopub.status.idle":"2022-03-30T02:34:41.226695Z","shell.execute_reply.started":"2022-03-30T02:34:41.212632Z","shell.execute_reply":"2022-03-30T02:34:41.225917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best estimator is `AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n                 max_depth=1, max_features='auto', min_impurity_decrease=0.25, \n                 random_state=441), learning_rate=0.5, n_estimators=1, random_state=441)`  \nBest scores and params are `-1.3915510004920844 {'learning_rate': 0.5, 'n_estimators': 1}`","metadata":{}},{"cell_type":"markdown","source":"# RandomForestClassifier","metadata":{}},{"cell_type":"code","source":"# X = train_data_clean_1.iloc[:,:-1]\n# y = train_data_clean_1['health']\n# X_train, X_test, y_train, y_test = train_test_sets(X,y, 0.8)\n\n# params = {'n_estimators':[250, 300], 'max_depth':[12, 15, 18],\\\n#          'min_samples_split':[0.01, 0],'bootstrap':[True, False]}\n# rfc_model = RandomForestClassifier(criterion='entropy', max_features=\"auto\", warm_start=True)\n# rfc = GridSearchCV(rfc_model, params, n_jobs=-1, verbose=0, refit='neg_log_loss', scoring=['neg_log_loss','accuracy'])\n# rfc.fit(X_train, y_train)\n# print(rfc.best_params_, rfc.best_score_, rfc.best_estimator_, sep='\\n')\n# rfc.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:34:41.22788Z","iopub.execute_input":"2022-03-30T02:34:41.228611Z","iopub.status.idle":"2022-03-30T02:34:41.24154Z","shell.execute_reply.started":"2022-03-30T02:34:41.228573Z","shell.execute_reply":"2022-03-30T02:34:41.240713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best estimator is `RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=15,\n                       min_impurity_decrease=0, min_samples_split=0.01,\n                       n_estimators=255, warm_start=True)`  \nBest score and params are `\n{'bootstrap': False, 'max_depth': 15, 'min_impurity_decrease': 0, 'min_samples_split': 0.01, 'n_estimators': 255, 'warm_start': True}\n-1.2343588039175948`","metadata":{}},{"cell_type":"markdown","source":"# AdaBoost + RandomForest","metadata":{}},{"cell_type":"code","source":"# X = train_data_clean_1.iloc[:,:-1]\n# y = train_data_clean_1['health']\n# X_train, X_test, y_train, y_test = train_test_sets(X,y, 0.8)\n\n# params = {'n_estimators':[1, 10, 20, 50], 'learning_rate':[0.1, 0.2, 0.25, 0.5]}\n# ada_rfc_model = AdaBoostClassifier(RandomForestClassifier(criterion='entropy', max_features=\"auto\"), random_state=441)\n# ada_rfc = GridSearchCV(ada_rfc_model, params, n_jobs=-1, verbose=0, scoring='neg_log_loss')\n# ada_rfc.fit(X_train, y_train)\n# print(ada_rfc.best_params_, ada_rfc.best_score_, ada_rfc.best_estimator_, sep='\\n')\n# ada_rfc.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T02:34:41.243148Z","iopub.execute_input":"2022-03-30T02:34:41.243442Z","iopub.status.idle":"2022-03-30T02:34:41.254334Z","shell.execute_reply.started":"2022-03-30T02:34:41.243411Z","shell.execute_reply":"2022-03-30T02:34:41.253301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best estimator is `AdaBoostClassifier(base_estimator=RandomForestClassifier(criterion='entropy'),\n                   learning_rate=0.1, n_estimators=1, random_state=441)`  \nBest params and score are `{'learning_rate': 0.1, 'n_estimators': 1} -1.2836877005693468`\n","metadata":{}},{"cell_type":"markdown","source":"# Neural Network  \n\nIt's not worth it for this competition","metadata":{}},{"cell_type":"code","source":"# X = train_data_clean_1.iloc[:,:-1]\n# y = train_data_clean_1['health']\n# X_train, X_test, y_train, y_test = train_test_sets(X,y, 0.8)\n# X_keras = X_train.to_numpy()\n\n# # encode class values as integers\n# encoder = LabelEncoder()\n# encoder.fit(y_train)\n# y_keras = encoder.transform(y_train)\n# # convert integers to dummy variables (i.e. one hot encoded)\n# y_keras = np_utils.to_categorical(y_keras)\n\n# # es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patient=100, min_delta=1)\n\n# neural_model = Sequential()\n# # neural_model.add(Dropout(0.4, input_dim=180))\n# neural_model.add(Dense(350, activation='tanh'))\n# # neural_model.add(Dropout(0.2))\n# neural_model.add(Dense(350, activation='softplus'))\n# # neural_model.add(Dropout(0.2))\n# neural_model.add(Dense(350, activation='exponential'))\n# neural_model.add(Dense(5, activation='softmax'))\n# neural_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # used the Mean Squared Error metric\n# fitted = neural_model.fit(X_keras, y_keras, epochs=250, batch_size=1250, verbose=0) # fit the model\n# print(f'{neural_model.evaluate(X_keras,y_keras, verbose=0)}') # Print the training MSE\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:34:53.534796Z","iopub.execute_input":"2022-04-02T19:34:53.53528Z","iopub.status.idle":"2022-04-02T19:36:16.862285Z","shell.execute_reply.started":"2022-04-02T19:34:53.535249Z","shell.execute_reply":"2022-04-02T19:36:16.86151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f'{neural_model.evaluate(X_keras,y_keras, verbose=0)}') # Print the training MSE\n# y_pred = neural_model.predict(X_test) # find predictions\n# encoder.fit(y_test)\n# # encode class values as integers\n# encoder = LabelEncoder()\n# encoder.fit(y_test)\n# y_keras_test = encoder.transform(y_test)\n# y_keras_test = np_utils.to_categorical(y_keras_test)\n# cce = CategoricalCrossentropy(from_logits=False)\n# cce(y_keras_test[:,1:], y_pred).numpy()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:36:56.608112Z","iopub.execute_input":"2022-04-02T19:36:56.608997Z","iopub.status.idle":"2022-04-02T19:36:57.803485Z","shell.execute_reply.started":"2022-04-02T19:36:56.608947Z","shell.execute_reply":"2022-04-02T19:36:57.800263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking  ","metadata":{}},{"cell_type":"code","source":"X = train_data_cleanX = train_data_clean_1.iloc[:,:-1]\ny = train_data_clean_1['health']\nX_train, X_test, y_train, y_test = train_test_sets(X,y)\n\nencoder = LabelEncoder()\nencoder.fit(y_train)\nytrain = encoder.transform(y_train)\nendoder = LabelEncoder()\nencoder.fit(y_test)\nytest = encoder.transform(y_test)\n\n\nrf = RandomForestClassifier(criterion='entropy', max_depth=18,\n                        n_estimators=250, random_state=441, min_samples_split= 0.01,)\nlog = LogisticRegression(C=0.1, l1_ratio=0.1, max_iter=5000, penalty='elasticnet',\n                   multi_class='multinomial', solver='saga', warm_start=True)\nxgb = XGBClassifier(base_score=0.5, booster='gbtree',\n              eval_metric='mlogloss', gamma=0, learning_rate=0.2,\n              max_depth = 2, n_estimators=100, objective='multi:softprob',\n              reg_alpha=0, reg_lambda=1, use_label_encoder=False, verbosity=0, random_state=441)\netc = ExtraTreesClassifier(criterion='entropy', max_depth=18, min_samples_split=0.01,\n                     n_estimators=325)\nest = [('rf', rf), ('log', log), ('xgb', xgb), ('etc', etc)]\nstack = StackingClassifier(estimators=est,\\\n              final_estimator= xgb, cv=10, passthrough=False, verbose=1)\nstack.fit(X_train, ytrain)\nprint(stack.estimators_, stack.final_estimator_, stack.stack_method_, sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:40:03.69948Z","iopub.execute_input":"2022-04-02T19:40:03.699795Z","iopub.status.idle":"2022-04-02T21:43:18.768034Z","shell.execute_reply.started":"2022-04-02T19:40:03.69975Z","shell.execute_reply":"2022-04-02T21:43:18.766835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(stack.score(X_test, ytest))\nfrom sklearn.metrics import log_loss, accuracy_score\n# from collections import Counter\n# pred = stack.predict(X_test)\nlog_loss(ytest,stack.predict_proba(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T21:55:48.507803Z","iopub.execute_input":"2022-04-02T21:55:48.508163Z","iopub.status.idle":"2022-04-02T21:55:49.839201Z","shell.execute_reply.started":"2022-04-02T21:55:48.50813Z","shell.execute_reply":"2022-04-02T21:55:49.832157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing for Submission","metadata":{}},{"cell_type":"code","source":"# test_data.info(verbose=True, show_counts=True)\ncols = train_data_clean_1.columns\nsubmission = pd.DataFrame()\nsubmission = pd.concat([submission, test_data.uniqueid], axis=1)\ntest_data_imp = test_data.copy()\nfor col in test_data_imp.columns:\n    if col in cols:\n        test_data_imp.loc[test_data_imp[col].isnull(), col] = test_data_imp[col].mode()[0]\n#         test_data_imp.loc[test_data_imp[col].isnull(), col] = floor(test_data_imp[col].median())\n    else:\n        test_data_imp.drop([col], axis=1,inplace=True)\nprint(test_data_imp.shape)\n# test_data.info(verbose=True,show_counts=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final_model =AdaBoostClassifier(DecisionTreeClassifier(),random_state=441, learning_rate=0.5, n_estimators=1)\n# Final_model = RandomForestClassifier(criterion='entropy', max_depth=10,\n#                        min_impurity_decrease=0.75, min_samples_split=0.5,\n#                        n_estimators=250)\n# Final_model = AdaBoostClassifier(base_estimator=RandomForestClassifier(criterion='entropy'),\n#                    learning_rate=0.5, n_estimators=1, random_state=441)\n# Final_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#               colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n#               eval_metric='mlogloss', gamma=0, gpu_id=-1, importance_type=None,\n#               interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n#               max_depth=2, min_child_weight=1,\n#               monotone_constraints='()', n_estimators=100, n_jobs=-1,\n#               num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n#               random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n#               subsample=1, tree_method='exact', validate_parameters=1,\n#               verbosity=0)\n# Final_model = RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=18,\n#                        min_impurity_decrease=0, min_samples_split=0.01,\n#                        n_estimators=255, warm_start=True)\n\nFinal_model = StackingClassifier(estimators=est,\\\n              final_estimator= xgb, cv=10, passthrough=False, verbose=1)\nX = train_data_clean_1.iloc[:,:-1]\ny = train_data_clean_1['health']\nencoder = LabelEncoder()\nencoder.fit(y)\nyenc = encoder.transform(y)\nFinal_model.fit(X,yenc)\nFinal_model.score(X,yenc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = Final_model.predict_proba(test_data_imp)\n# predictions\nfor i in range(5):\n    submission['p'+str(i+1)] = predictions[:,i]\nprint(submission.sample(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('/kaggle/working/submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}